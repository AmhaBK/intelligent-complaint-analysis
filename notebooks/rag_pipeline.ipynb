{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39924e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2294aa25f92a4dbb8b855a4c385c0acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fa06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "#  Config paths\n",
    "\n",
    "FAISS_INDEX_PATH = \"../vector_store/faiss_recursive_sample.index\"\n",
    "METADATA_PATH = \"../vector_store/recursive_sample_metadata.pkl\"\n",
    "\n",
    "\n",
    "#  Load index and metadata\n",
    "\n",
    "print(\"ðŸ”„ Loading FAISS index...\")\n",
    "index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "\n",
    "print(\"ðŸ”„ Loading metadata...\")\n",
    "with open(METADATA_PATH, \"rb\") as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "print(f\"âœ… Loaded index with {index.ntotal} vectors.\")\n",
    "print(f\"âœ… Loaded metadata with {len(metadata)} records.\")\n",
    "\n",
    "\n",
    "#  Load embedding model\n",
    "\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "#  Set up Hugging Face Inference API for Mistral\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "\n",
    "client = InferenceClient(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    provider='auto'  # Mistral model on Hugging Face\n",
    ")\n",
    "\n",
    "\n",
    "#  Define RAG answer function\n",
    "\n",
    "def generate_answer(query, top_k=5):\n",
    "    # Embed the query\n",
    "    query_embedding = embedder.encode([query])\n",
    "    \n",
    "    # Perform similarity search\n",
    "    D, I = index.search(np.array(query_embedding).astype(\"float32\"), top_k)\n",
    "    retrieved_chunks = [metadata[i] for i in I[0]]\n",
    "    \n",
    "    context_text = \"\\n\\n\".join(\n",
    "        [chunk.get(\"chunk\", \"N/A\") for chunk in retrieved_chunks]\n",
    "    )\n",
    "\n",
    "    # Prompt template\n",
    "    prompt = f\"\"\"\n",
    "You are a financial analyst assistant for CrediTrust. Your task is to answer questions about customer complaints.\n",
    "Use the following retrieved complaint excerpts to formulate your answer.\n",
    "If the context does not contain the answer, say you don't have enough information.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    # Run Mistral on Hugging Face Inference API\n",
    "    response = client.text_generation(\n",
    "        prompt,\n",
    "    \n",
    "        max_new_tokens=256,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    return response, retrieved_chunks\n",
    "\n",
    "\n",
    "# Test\n",
    "\n",
    "question = \"Why are people unhappy with their credit cards?\"\n",
    "answer, retrieved_sources = generate_answer(question)\n",
    "\n",
    "print(\"\\nâœ… AI Answer:\")\n",
    "print(answer)\n",
    "\n",
    "print(\"\\nðŸ” Retrieved Chunks (first 2 shown):\")\n",
    "for i, chunk in enumerate(retrieved_sources[:2]):\n",
    "    print(f\"#{i+1}: {chunk['chunk'][:200]}...\")  # Show first 200 chars\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
